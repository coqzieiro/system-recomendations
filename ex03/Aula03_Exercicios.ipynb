{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a73ef97",
   "metadata": {},
   "source": [
    "## Aula 03 - Filtragem Baseada em Conteúdo - Exercícios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fc65b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31c492",
   "metadata": {},
   "source": [
    "### Importar base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e33e890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [....................................................] 65019041 / 65019041\n",
      "Saved under ml-20m-compact.tar (1).gz\n",
      "dataset/\n",
      "dataset/tags_sample.csv\n",
      "dataset/._.DS_Store\n",
      "dataset/.DS_Store\n",
      "dataset/movies_sample.csv\n",
      "dataset/._genome-tags.csv\n",
      "dataset/genome-tags.csv\n",
      "dataset/._ml-youtube.csv\n",
      "dataset/ml-youtube.csv\n",
      "dataset/._genome-scores.csv\n",
      "dataset/genome-scores.csv\n",
      "dataset/ratings_sample.csv\n"
     ]
    }
   ],
   "source": [
    "import wget\n",
    "!python3 -m wget https://github.com/mmanzato/MBABigData/raw/master/ml-20m-compact.tar.gz\n",
    "!tar -xvzf ml-20m-compact.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1aa3f489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>7481</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Enemy Mine (1985)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1046</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Beautiful Thing (1996)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>616</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Aristocats, The (1970)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>3535</td>\n",
       "      <td>2.0</td>\n",
       "      <td>American Psycho (2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>5669</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Bowling for Columbine (2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190616</th>\n",
       "      <td>138493</td>\n",
       "      <td>288</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Natural Born Killers (1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190617</th>\n",
       "      <td>138493</td>\n",
       "      <td>1748</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dark City (1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190618</th>\n",
       "      <td>138493</td>\n",
       "      <td>616</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Aristocats, The (1970)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190619</th>\n",
       "      <td>138493</td>\n",
       "      <td>1597</td>\n",
       "      <td>4.5</td>\n",
       "      <td>Conspiracy Theory (1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190620</th>\n",
       "      <td>138493</td>\n",
       "      <td>7371</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Dogville (2003)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190621 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        userId  movieId  rating                         title\n",
       "0           11     7481     5.0             Enemy Mine (1985)\n",
       "1           11     1046     4.5        Beautiful Thing (1996)\n",
       "2           11      616     4.0        Aristocats, The (1970)\n",
       "3           11     3535     2.0        American Psycho (2000)\n",
       "4           11     5669     5.0  Bowling for Columbine (2002)\n",
       "...        ...      ...     ...                           ...\n",
       "190616  138493      288     5.0   Natural Born Killers (1994)\n",
       "190617  138493     1748     5.0              Dark City (1998)\n",
       "190618  138493      616     4.0        Aristocats, The (1970)\n",
       "190619  138493     1597     4.5      Conspiracy Theory (1997)\n",
       "190620  138493     7371     5.0               Dogville (2003)\n",
       "\n",
       "[190621 rows x 4 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_csv('./dataset/movies_sample.csv')\n",
    "ratings = pd.read_csv('./dataset/ratings_sample.csv')\n",
    "df = ratings[['userId', 'movieId', 'rating']]\n",
    "df = df.merge(movies[['movieId', 'title']])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bd9ec33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>tag</th>\n",
       "      <th>timestamp_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>279</td>\n",
       "      <td>916</td>\n",
       "      <td>Gregory Peck</td>\n",
       "      <td>1329962459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>279</td>\n",
       "      <td>916</td>\n",
       "      <td>need to own</td>\n",
       "      <td>1329962471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>279</td>\n",
       "      <td>916</td>\n",
       "      <td>romantic comedy</td>\n",
       "      <td>1329962476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>279</td>\n",
       "      <td>916</td>\n",
       "      <td>Rome</td>\n",
       "      <td>1329962490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>279</td>\n",
       "      <td>916</td>\n",
       "      <td>royalty</td>\n",
       "      <td>1329962474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId              tag  timestamp_y\n",
       "0     279      916     Gregory Peck   1329962459\n",
       "1     279      916      need to own   1329962471\n",
       "2     279      916  romantic comedy   1329962476\n",
       "3     279      916             Rome   1329962490\n",
       "4     279      916          royalty   1329962474"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_tags = pd.read_csv('./dataset/tags_sample.csv')\n",
    "movies_tags.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2def9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_users = {user: idx for idx, user in enumerate(df.userId.unique())}\n",
    "map_items = {item: idx for idx, item in enumerate(df.movieId.unique())}\n",
    "df['userId'] = df['userId'].map(map_users)\n",
    "df['movieId'] = df['movieId'].map(map_items)\n",
    "movies_tags['userId'] = movies_tags['userId'].map(map_users)\n",
    "movies_tags['movieId'] = movies_tags['movieId'].map(map_items)\n",
    "map_title = {}\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    map_title[row.movieId] = row.title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c20039f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>tag</th>\n",
       "      <th>timestamp_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>Gregory Peck</td>\n",
       "      <td>1329962459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>need to own</td>\n",
       "      <td>1329962471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>romantic comedy</td>\n",
       "      <td>1329962476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>Rome</td>\n",
       "      <td>1329962490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>royalty</td>\n",
       "      <td>1329962474</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   userId  movieId              tag  timestamp_y\n",
       "0      18       34     Gregory Peck   1329962459\n",
       "1      18       34      need to own   1329962471\n",
       "2      18       34  romantic comedy   1329962476\n",
       "3      18       34             Rome   1329962490\n",
       "4      18       34          royalty   1329962474"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_tags.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9936409b",
   "metadata": {},
   "source": [
    "### Divisão da base em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a43fa3d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size=.2, random_state=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6be08",
   "metadata": {},
   "source": [
    "***Exercício 01:*** Aplique a filtragem baseada em conteúdo (ItemAttributeKNN do CaseRecommender) com as tags associadas aos filmes da base. Utilize Jaccard como métrica de similaridade e k=5 vizinhos para predição.\n",
    "\n",
    "Documentação do ItemAttributeKNN: https://github.com/caserec/CaseRecommender/blob/master/caserec/recommenders/rating_prediction/item_attribute_knn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "caca125b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 0.8009755521361313\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# dicionário {item: [tags]}\n",
    "tag_by_item = defaultdict(list)\n",
    "for _, row in movies_tags.iterrows():\n",
    "    tag_by_item[row.movieId].append(row.tag)\n",
    "\n",
    "# similaridade Jaccard\n",
    "def jaccard(a, b):\n",
    "    A, B = set(a), set(b)\n",
    "    return len(A & B) / len(A | B) if A or B else 0\n",
    "\n",
    "# matriz de similaridade entre itens\n",
    "n_items = df.movieId.max() + 1\n",
    "S = np.zeros((n_items, n_items))\n",
    "for i in range(n_items):\n",
    "    for j in range(i + 1, n_items):\n",
    "        S[i, j] = S[j, i] = jaccard(tag_by_item[i], tag_by_item[j])\n",
    "\n",
    "# predição com k vizinhos\n",
    "def predict(uid, iid, k=5):\n",
    "    user_hist = train[train.userId == uid]\n",
    "    if user_hist.empty:\n",
    "        return 3.0  # fallback neutro\n",
    "\n",
    "    # considere apenas vizinhos com similaridade > 0\n",
    "    sims = [(S[iid, j], r) for j, r in zip(user_hist.movieId, user_hist.rating) if S[iid, j] > 0]\n",
    "    sims.sort(reverse=True)\n",
    "    sims = sims[:k]\n",
    "\n",
    "    if not sims:\n",
    "        # se não há vizinhos semelhantes, cai para a média do usuário\n",
    "        return float(user_hist.rating.mean())\n",
    "\n",
    "    num = sum(s * r for s, r in sims)\n",
    "    den = sum(s for s, _ in sims)\n",
    "\n",
    "    return num / den if den != 0 else float(user_hist.rating.mean())\n",
    "\n",
    "# gera predições para o conjunto de teste\n",
    "preds = [predict(u, i) for u, i in zip(test.userId, test.movieId)]\n",
    "\n",
    "# métrica de erro\n",
    "mae = np.mean(np.abs(test.rating - preds))\n",
    "print(\"MAE:\", mae)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e8b11",
   "metadata": {},
   "source": [
    "### Preparação para o exercício 2 - Download e extração de metadados multimídia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5458bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% [......................................................] 5996435 / 5996435\n",
      "Saved under ml-20m-features.tar (1).gz\n",
      "features/\n",
      "features/._m4infus_max_histogram_300_sn.arq\n",
      "features/m4infus_max_histogram_300_sn.arq\n",
      "features/._mm_avg_histogram_100_sn.arq\n",
      "features/mm_avg_histogram_100_sn.arq\n",
      "features/._visual_histogram_100_sn.arq\n",
      "features/visual_histogram_100_sn.arq\n",
      "features/._visual_histogram_50_sn.arq\n",
      "features/visual_histogram_50_sn.arq\n",
      "features/._aural_histogram_50.arq\n",
      "features/aural_histogram_50.arq\n",
      "features/._mm_max_histogram_300.arq\n",
      "features/mm_max_histogram_300.arq\n",
      "features/._m4infus_max_histogram_50.arq\n",
      "features/m4infus_max_histogram_50.arq\n",
      "features/._mm_max_histogram_100.arq\n",
      "features/mm_max_histogram_100.arq\n",
      "features/._mm_max_histogram_50_sn.arq\n",
      "features/mm_max_histogram_50_sn.arq\n",
      "features/._visual_histogram_100.arq\n",
      "features/visual_histogram_100.arq\n",
      "features/._visual_histogram_300.arq\n",
      "features/visual_histogram_300.arq\n",
      "features/._aural_histogram_100_sn.arq\n",
      "features/aural_histogram_100_sn.arq\n",
      "features/._mm_avg_histogram_100.arq\n",
      "features/mm_avg_histogram_100.arq\n",
      "features/._mm_max_histogram_100_sn.arq\n",
      "features/mm_max_histogram_100_sn.arq\n",
      "features/._mm_sum_histogram_100_sn.arq\n",
      "features/mm_sum_histogram_100_sn.arq\n",
      "features/._mm_avg_histogram_300.arq\n",
      "features/mm_avg_histogram_300.arq\n",
      "features/._visual_histogram_300_sn.arq\n",
      "features/visual_histogram_300_sn.arq\n",
      "features/._mm_avg_histogram_300_sn.arq\n",
      "features/mm_avg_histogram_300_sn.arq\n",
      "features/._m4infus_max_histogram_100.arq\n",
      "features/m4infus_max_histogram_100.arq\n",
      "features/._m4infus_max_histogram_100_sn.arq\n",
      "features/m4infus_max_histogram_100_sn.arq\n",
      "features/._mm_sum_histogram_50_sn.arq\n",
      "features/mm_sum_histogram_50_sn.arq\n",
      "features/._m4infus_max_histogram_300.arq\n",
      "features/m4infus_max_histogram_300.arq\n",
      "features/._mm_avg_histogram_50_sn.arq\n",
      "features/mm_avg_histogram_50_sn.arq\n",
      "features/._mm_sum_histogram_50.arq\n",
      "features/mm_sum_histogram_50.arq\n",
      "features/._visual_histogram_50.arq\n",
      "features/visual_histogram_50.arq\n",
      "features/._aural_histogram_50_sn.arq\n",
      "features/aural_histogram_50_sn.arq\n",
      "features/._mm_sum_histogram_300.arq\n",
      "features/mm_sum_histogram_300.arq\n",
      "features/._m4infus_max_histogram_50_sn.arq\n",
      "features/m4infus_max_histogram_50_sn.arq\n",
      "features/._mm_max_histogram_50.arq\n",
      "features/mm_max_histogram_50.arq\n",
      "features/._mm_avg_histogram_50.arq\n",
      "features/mm_avg_histogram_50.arq\n",
      "features/._mm_max_histogram_300_sn.arq\n",
      "features/mm_max_histogram_300_sn.arq\n",
      "features/._mm_sum_histogram_300_sn.arq\n",
      "features/mm_sum_histogram_300_sn.arq\n",
      "features/._mm_sum_histogram_100.arq\n",
      "features/mm_sum_histogram_100.arq\n",
      "features/._aural_histogram_300.arq\n",
      "features/aural_histogram_300.arq\n",
      "features/._aural_histogram_300_sn.arq\n",
      "features/aural_histogram_300_sn.arq\n",
      "features/._aural_histogram_100.arq\n",
      "features/aural_histogram_100.arq\n"
     ]
    }
   ],
   "source": [
    "!python3 -m wget https://github.com/mmanzato/MBABigData/raw/master/ml-20m-features.tar.gz\n",
    "! tar -xvzf ml-20m-features.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76f2c0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. movies: 433\n",
      "Size of each word: 50\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open('./features/visual_histogram_50.arq', 'rb') as arq_visualHistograms:\n",
    "    visualHistograms = pickle.load(arq_visualHistograms)\n",
    "print('No. movies: ' + str(len(visualHistograms)))\n",
    "print('Size of each word: ' + str(len(visualHistograms[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be058141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00948992, 0.00237248, 0.00355872, 0.01304864, 0.05338078,\n",
       "       0.00711744, 0.05456702, 0.00355872, 0.0059312 , 0.00355872,\n",
       "       0.0059312 , 0.00474496, 0.00711744, 0.04507711, 0.07591934,\n",
       "       0.00355872, 0.1316726 , 0.05338078, 0.00830368, 0.        ,\n",
       "       0.01304864, 0.00474496, 0.02253855, 0.00355872, 0.02728351,\n",
       "       0.00237248, 0.02016607, 0.00830368, 0.0059312 , 0.00237248,\n",
       "       0.00355872, 0.04033215, 0.01067616, 0.00237248, 0.00830368,\n",
       "       0.05931198, 0.0059312 , 0.01067616, 0.02965599, 0.01423488,\n",
       "       0.00711744, 0.02609727, 0.        , 0.00237248, 0.01423488,\n",
       "       0.03084223, 0.02016607, 0.00118624, 0.08778173, 0.02253855])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualHistograms[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae2452",
   "metadata": {},
   "source": [
    "***Exercício 02:*** Como visto, o algoritmo ItemAttributeKNN pode ser usado com diferentes tipos de metadados, como gêneros, tags e palavras no geral. Mais do que isso, podemos adaptá-lo também para que a similaridade entre itens seja feita com base em informações multimídia, como imagens, áudio, etc. \n",
    "\n",
    "A base de dados utilizada até o momento, ml-20m-compact.tar.gz possui, além das interações de usuários com filmes, uma série de arquivos que contém informações multimídia que foram extraídas dos trailers de cada filme. Esses arquivos estão condensados no zip ml-20m-features.tar.gz, o qual foi feito o download e extraído acima. \n",
    "\n",
    "Considere por exemplo o arquivo visual_histogram_50.arq. Ele possui 433 vetores (no. de filmes) de tamanho 50. Podemos pensar que cada vetor desse representa informações visuais (cor, brilho, imagem, etc.) que foram extraídas dos trailers de cada filme. \n",
    "\n",
    "Sua tarefa é usar esses vetores de características visuais no cálculo de similaridade entre os filmes, e em seguida, aplicar essas similaridades no algoritmo ItemAttributeKNN para gerar recomendações. \n",
    "\n",
    "Dica 1: para calcular a similaridade entre dois vetores pode-se usar o ângulo de cosseno (vide https://en.wikipedia.org/wiki/Cosine_similarity). \n",
    "\n",
    "Dica 2: é possível passar para o algoritmo ItemAttributeKNN a matriz de similaridade entre itens, por meio do parâmetro similarity_file=arquivo. Veja em: https://github.com/caserec/CaseRecommender/blob/master/caserec/recommenders/rating_prediction/item_attribute_knn.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a764c634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE (visual): 0.805868\n",
      "RMSE (visual): 1.036540\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "# 1) Matriz de características visuais (já carregada como 'visualHistograms')\n",
    "M = np.array(visualHistograms, dtype=float) \n",
    "n_feat_items = M.shape[0]\n",
    "\n",
    "# 2) Similaridade por cosseno entre itens-com-feature\n",
    "X = M / np.clip(norm(M, axis=1, keepdims=True), 1e-12, None)\n",
    "S_visual_small = X @ X.T\n",
    "np.clip(S_visual_small, -1.0, 1.0, out=S_visual_small)\n",
    "np.fill_diagonal(S_visual_small, 1.0)\n",
    "\n",
    "# 3) Embutir S_visual_small no espaço total de itens de df (pode haver tamanhos diferentes)\n",
    "n_items_total = int(df.movieId.max()) + 1\n",
    "S_visual = np.zeros((n_items_total, n_items_total), dtype=float)\n",
    "\n",
    "# usa apenas a interseção de índices válida para os dois lados\n",
    "m = min(n_feat_items, n_items_total)\n",
    "S_visual[:m, :m] = S_visual_small[:m, :m]\n",
    "\n",
    "# 4) Predição com K vizinhos usando S_visual (com proteções)\n",
    "def predict_visual(uid, iid, k=5):\n",
    "    user_hist = train[train.userId == uid]\n",
    "    if user_hist.empty:\n",
    "        return 3.0  # fallback neutro\n",
    "\n",
    "    # só considera vizinhos com similaridade > 0\n",
    "    sims = []\n",
    "    for j, r in zip(user_hist.movieId, user_hist.rating):\n",
    "        s = S_visual[iid, j]\n",
    "        if s > 0:\n",
    "            sims.append((s, r))\n",
    "\n",
    "    sims.sort(key=lambda x: x[0], reverse=True)\n",
    "    sims = sims[:k]\n",
    "\n",
    "    if not sims:\n",
    "        # sem vizinhos visuais úteis → média do usuário\n",
    "        return float(user_hist.rating.mean())\n",
    "\n",
    "    num = sum(s * r for s, r in sims)\n",
    "    den = sum(s for s, _ in sims)\n",
    "    return (num / den) if den != 0 else float(user_hist.rating.mean())\n",
    "\n",
    "# 5) Avaliação no conjunto de teste\n",
    "preds_visual = [predict_visual(u, i) for u, i in zip(test.userId, test.movieId)]\n",
    "mae_visual = np.mean(np.abs(test.rating - preds_visual))\n",
    "rmse_visual = float(np.sqrt(np.mean((test.rating - preds_visual) ** 2)))\n",
    "print(f\"MAE (visual): {mae_visual:.6f}\")\n",
    "print(f\"RMSE (visual): {rmse_visual:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feaa7ce1",
   "metadata": {},
   "source": [
    "***Exercício 03:*** Implementar uma função que retorna a probabilidade de um item ser relevante para um usuário, considerando os gêneros dos filmes. Utilize métodos probabilísticos. \n",
    "- A partir do conjunto de treinamento, obter todas as interações do usuário u.\n",
    "- Rotular as notas desse usuário como: item relevante se nota >=3 e não relevante se nota < 3. \n",
    "- Dado um item do conjunto de teste, aplicar o Teorema de Bayes com suavização de Laplace. Utilizar os gêneros associados. \n",
    "- Retornar se o item é ou não relevante, e em seguida, comparar o resultado com a nota real que esse usuário deu para o item (disponível no conjunto de teste)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ddbbd877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user=1836 item=  60  P(relev)=0.931  pred=True  real=False\n",
      "user=8646 item=  33  P(relev)=0.879  pred=True  real=True\n",
      "user=1464 item=  19  P(relev)=0.993  pred=True  real=True\n",
      "user=5315 item=  33  P(relev)=0.497  pred=False  real=True\n",
      "user=6571 item=  18  P(relev)=0.808  pred=True  real=True\n",
      "Acurácia (teste): 0.7788\n",
      "Precision: 0.8136  Recall: 0.9354  F1: 0.8703\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Mapeia gêneros para os IDs INTERNOS (mesmo espaço de df/train/test)\n",
    "movies_mapped = movies.copy()\n",
    "movies_mapped['movieId'] = movies_mapped['movieId'].map(map_items)  # usa o mesmo map_items já criado\n",
    "movies_mapped = movies_mapped.dropna(subset=['movieId']).copy()\n",
    "movies_mapped['movieId'] = movies_mapped['movieId'].astype(int)\n",
    "\n",
    "# dicionário {iid_interno: \"Genre1|Genre2|...\"}\n",
    "map_genres = dict(zip(movies_mapped['movieId'], movies_mapped['genres']))\n",
    "\n",
    "# vocabulário de gêneros (V)\n",
    "all_genres = set()\n",
    "for g in movies_mapped['genres'].fillna(''):\n",
    "    for x in g.split('|'):\n",
    "        x = x.strip()\n",
    "        if x and x.lower() != '(no genres listed)':\n",
    "            all_genres.add(x)\n",
    "V = len(all_genres)\n",
    "\n",
    "def split_genres(iid):\n",
    "    \"\"\"Retorna lista de gêneros para o item interno iid; pode ser [].\"\"\"\n",
    "    gstr = map_genres.get(int(iid), '')\n",
    "    genres = [x for x in gstr.split('|') if x and x.lower() != '(no genres listed)']\n",
    "    return genres\n",
    "\n",
    "# 2) Treina contadores por usuário (histórico do treino)\n",
    "def train_user_model(uid):\n",
    "    user_hist = train[train.userId == uid][['movieId', 'rating']].copy()\n",
    "    model = {\"relev\": Counter(), \"n_relev\": Counter(), \"tot_r\": 0, \"tot_nr\": 0}\n",
    "\n",
    "    for _, row in user_hist.iterrows():\n",
    "        genres = split_genres(row.movieId)\n",
    "        if row.rating >= 3:\n",
    "            model[\"relev\"].update(genres)\n",
    "            model[\"tot_r\"] += 1\n",
    "        else:\n",
    "            model[\"n_relev\"].update(genres)\n",
    "            model[\"tot_nr\"] += 1\n",
    "    return model\n",
    "\n",
    "# 3) Probabilidade de relevância P(R=1 | gêneros do item) com Laplace (em log-space)\n",
    "def prob_relevant(uid, iid):\n",
    "    model = train_user_model(uid)\n",
    "    genres = split_genres(iid)\n",
    "\n",
    "    tot_r, tot_nr = model[\"tot_r\"], model[\"tot_nr\"]\n",
    "    N = tot_r + tot_nr\n",
    "\n",
    "    # priors com Laplace nos classes (beta(1,1))\n",
    "    p_r  = (tot_r + 1) / (N + 2)\n",
    "    p_nr = (tot_nr + 1) / (N + 2)\n",
    "\n",
    "    # caso extremo: usuário sem histórico → retorna 0.5 (neutro)\n",
    "    if N == 0:\n",
    "        return 0.5\n",
    "\n",
    "    # verossimilhanças com Laplace para cada gênero (Naive Bayes)\n",
    "    # log P(g|R) = log((count(g,R)+1)/(tot_r + V))\n",
    "    log_r  = np.log(p_r)\n",
    "    log_nr = np.log(p_nr)\n",
    "\n",
    "    for g in genres:\n",
    "        log_r  += np.log((model[\"relev\"][g]   + 1) / (tot_r  + V))\n",
    "        log_nr += np.log((model[\"n_relev\"][g] + 1) / (tot_nr + V))\n",
    "\n",
    "    # converte de volta para probabilidade com softmax de 2 classes\n",
    "    m = max(log_r, log_nr)  # estabilidade numérica\n",
    "    pr  = np.exp(log_r  - m)\n",
    "    pnr = np.exp(log_nr - m)\n",
    "    return float(pr / (pr + pnr))\n",
    "\n",
    "def is_relevant(uid, iid, threshold=0.5):\n",
    "    \"\"\"Classifica como relevante (True) se prob >= limiar.\"\"\"\n",
    "    return prob_relevant(uid, iid) >= threshold\n",
    "\n",
    "# 4) Exemplo em 5 itens do teste (predição vs rótulo real)\n",
    "for _, row in test.head(5).iterrows():\n",
    "    p = prob_relevant(row.userId, row.movieId)\n",
    "    print(f\"user={row.userId:4d} item={row.movieId:4d}  P(relev)={p:.3f}  pred={p>=0.5}  real={row.rating>=3}\")\n",
    "\n",
    "# 5) Avaliação simples no conjunto de teste (acurácia)\n",
    "y_true = (test['rating'] >= 3).to_numpy()\n",
    "y_pred = np.array([is_relevant(u, i) for u, i in zip(test.userId, test.movieId)])\n",
    "acc = (y_true == y_pred).mean()\n",
    "print(f\"Acurácia (teste): {acc:.4f}\")\n",
    "\n",
    "# (Opcional) Métricas adicionais\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "print(f\"Precision: {prec:.4f}  Recall: {rec:.4f}  F1: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b4fc7d",
   "metadata": {},
   "source": [
    "***Exercício 04:*** No notebook de exemplos, existe uma implementação de Filtragem Baseada em Conteúdo usando Multi-Layer Perceptron como um regressor (MLPRegressor) que prevê a nota de cada usuário para filmes ainda não vistos. O algoritmo retorna uma lista de top K filmes com maiores notas. O treinamento é realizado utilizando as notas que o usuário deu para os filmes e seus respectivos gêneros.\n",
    "- Usando a classe MLPClassifier (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html), implemente uma versão que classifica os filmes não vistos como relevante ou não-relevante.\n",
    "- No conjunto de dados, realize a binarização das notas, de modo que notas acima de 3 são relevantes e notas abaixo de 3 são não-relevantes.\n",
    "- Retorne os top k filmes mais relevantes.\n",
    "- Para um usuário qualquer da base (ou algum outro usuário fictício), analise subjetivamente a qualidade das recomendações, comparando os modelos MLPRegressor e MLPClassifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9998dd70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usuário 4926 — TOP-10 (MLPClassifier: prob. de relevância)\n",
      "   0.654  | Fata Morgana (1971)\n",
      "   0.645  | River Queen (2005)\n",
      "   0.645  | Australia (2008)\n",
      "   0.638  | Heroes of Telemark, The (1965) \n",
      "   0.635  | Across the Sea of Time (1995)\n",
      "   0.631  | Interstellar (2014)\n",
      "   0.627  | Fist of Fury (Chinese Connection, The) (Jing wu men) (1972)\n",
      "   0.627  | First on the Moon (Pervye na Lune) (2005)\n",
      "   0.627  | Mars (1968)\n",
      "   0.623  | Family, The (Famiglia, La) (1987)\n",
      "\n",
      "Usuário 4926 — TOP-10 (MLPRegressor: nota prevista)\n",
      "   4.807  | River Queen (2005)\n",
      "   4.448  | Alphaville (Alphaville, une étrange aventure de Lemmy Caution) (1965)\n",
      "   4.258  | Australia (2008)\n",
      "   4.229  | Balance (1989)\n",
      "   4.176  | Patema Inverted (2013)\n",
      "   4.032  | Fist of Fury (Chinese Connection, The) (Jing wu men) (1972)\n",
      "   4.002  | Cowboy and the Lady, The (1938)\n",
      "   3.961  | Jack and the Cuckoo-Clock Heart (Jack et la mécanique du coeur) (2013)\n",
      "   3.902  | Captain America (1990)\n",
      "   3.893  | 21 (2008)\n",
      "\n",
      "Acurácia no TESTE do usuário 4926: 0.7500 | Precision: 0.8571 | Recall: 0.8571 | F1: 0.8571\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "# ===== 1) Preparar matriz de features por item (gêneros) no ESPAÇO DE IDs INTERNOS =====\n",
    "movies_mapped = movies.copy()\n",
    "movies_mapped['movieId'] = movies_mapped['movieId'].map(map_items)  # mapeia p/ ids internos\n",
    "movies_mapped = movies_mapped.dropna(subset=['movieId']).copy()\n",
    "movies_mapped['movieId'] = movies_mapped['movieId'].astype(int)\n",
    "\n",
    "# binarização de gêneros\n",
    "movies_mapped['genres_split'] = movies_mapped['genres'].fillna('(no genres listed)').apply(lambda x: [g for g in x.split('|') if g and g.lower() != '(no genres listed)'])\n",
    "mlb = MultiLabelBinarizer()\n",
    "X_all_items_mlb = mlb.fit_transform(movies_mapped['genres_split'])\n",
    "\n",
    "# cria uma matriz de features indexada por ID interno (linha = movieId interno)\n",
    "n_items_total = int(df.movieId.max()) + 1\n",
    "n_feats = X_all_items_mlb.shape[1]\n",
    "X_items = np.zeros((n_items_total, n_feats), dtype=np.float32)\n",
    "\n",
    "id_order = movies_mapped['movieId'].to_numpy()\n",
    "X_items[id_order] = X_all_items_mlb  # injeta as linhas nas posições corretas\n",
    "\n",
    "# util: título por id\n",
    "title_by_id = {int(mid): t for mid, t in zip(movies_mapped['movieId'], movies_mapped['title'])}\n",
    "def title(iid): return title_by_id.get(int(iid), f\"item {int(iid)}\")\n",
    "\n",
    "# ===== 2) Construir dados por usuário =====\n",
    "def build_user_xy(uid):\n",
    "    \"\"\"Retorna X_class, y_class (binário), X_reg, y_reg usando histórico do TREINO.\"\"\"\n",
    "    hist = train[train.userId == uid][['movieId', 'rating']]\n",
    "    if hist.empty:\n",
    "        return None, None, None, None\n",
    "\n",
    "    X_list_c = []\n",
    "    y_list_c = []\n",
    "    X_list_r = []\n",
    "    y_list_r = []\n",
    "\n",
    "    for _, row in hist.iterrows():\n",
    "        iid = int(row.movieId)\n",
    "        X_list_c.append(X_items[iid])\n",
    "        y_list_c.append(1 if row.rating >= 3 else 0)\n",
    "        X_list_r.append(X_items[iid])\n",
    "        y_list_r.append(float(row.rating))\n",
    "\n",
    "    X_class = np.vstack(X_list_c) if X_list_c else None\n",
    "    y_class = np.array(y_list_c, dtype=int) if y_list_c else None\n",
    "    X_reg   = np.vstack(X_list_r) if X_list_r else None\n",
    "    y_reg   = np.array(y_list_r, dtype=float) if y_list_r else None\n",
    "    return X_class, y_class, X_reg, y_reg\n",
    "\n",
    "def candidate_items_for_user(uid):\n",
    "    seen = set(train.loc[train.userId == uid, 'movieId'].tolist())\n",
    "    all_items = set(range(n_items_total))\n",
    "    return sorted(list(all_items - seen))\n",
    "\n",
    "# ===== 3) Treinar modelos por usuário e recomendar =====\n",
    "def recommend_for_user(uid, k=10, hidden=(64,), random_state=42):\n",
    "    Xc, yc, Xr, yr = build_user_xy(uid)\n",
    "\n",
    "    # candidatos (não vistos no TREINO)\n",
    "    cands = candidate_items_for_user(uid)\n",
    "    if not cands:\n",
    "        return [], []\n",
    "\n",
    "    X_cands = X_items[cands]\n",
    "\n",
    "    # ---------- Classifier (relevante vs não) ----------\n",
    "    clf_scores = None\n",
    "    if Xc is not None and len(np.unique(yc)) >= 2:\n",
    "        clf = MLPClassifier(\n",
    "            hidden_layer_sizes=hidden,\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=1e-4,\n",
    "            learning_rate_init=1e-3,\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=15,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        clf.fit(Xc, yc)\n",
    "        clf_scores = clf.predict_proba(X_cands)[:, 1]  # P(relevante)\n",
    "\n",
    "    # se não der para treinar (poucas amostras ou 1 classe), cai em baseline simples (0.5)\n",
    "    if clf_scores is None:\n",
    "        clf_scores = np.full(len(cands), 0.5, dtype=float)\n",
    "\n",
    "    # Top-K por probabilidade\n",
    "    topk_idx_clf = np.argsort(clf_scores)[-k:][::-1]\n",
    "    top_clf = [(int(cands[i]), float(clf_scores[i])) for i in topk_idx_clf]\n",
    "\n",
    "    # ---------- Regressor (nota prevista) ----------\n",
    "    reg_scores = None\n",
    "    if Xr is not None and len(Xr) >= 2:\n",
    "        reg = MLPRegressor(\n",
    "            hidden_layer_sizes=hidden,\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=1e-4,\n",
    "            learning_rate_init=1e-3,\n",
    "            max_iter=300,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change=15,\n",
    "            random_state=random_state\n",
    "        )\n",
    "        reg.fit(Xr, yr)\n",
    "        reg_scores = reg.predict(X_cands)\n",
    "\n",
    "    # fallback: se não deu para treinar (muito pouco dado), usa média do usuário\n",
    "    if reg_scores is None:\n",
    "        user_mean = float(train.loc[train.userId == uid, 'rating'].mean()) if not train.loc[train.userId == uid].empty else 3.0\n",
    "        reg_scores = np.full(len(cands), user_mean, dtype=float)\n",
    "\n",
    "    # Top-K por nota prevista\n",
    "    topk_idx_reg = np.argsort(reg_scores)[-k:][::-1]\n",
    "    top_reg = [(int(cands[i]), float(reg_scores[i])) for i in topk_idx_reg]\n",
    "\n",
    "    return top_clf, top_reg\n",
    "\n",
    "# ===== 4) Rodar para um usuário e imprimir Top-K =====\n",
    "k = 10\n",
    "uid = int(train.userId.sample(1, random_state=1).iloc[0])  # escolha um usuário qualquer do treino\n",
    "\n",
    "top_clf, top_reg = recommend_for_user(uid, k=k, hidden=(64,), random_state=42)\n",
    "\n",
    "print(f\"Usuário {uid} — TOP-{k} (MLPClassifier: prob. de relevância)\")\n",
    "for iid, p in top_clf:\n",
    "    print(f\"  {p:6.3f}  | {title(iid)}\")\n",
    "\n",
    "print(f\"\\nUsuário {uid} — TOP-{k} (MLPRegressor: nota prevista)\")\n",
    "for iid, s in top_reg:\n",
    "    print(f\"  {s:6.3f}  | {title(iid)}\")\n",
    "\n",
    "# (Opcional) Avaliação simples no conjunto de TESTE — acurácia do classificador em itens do TESTE deste usuário\n",
    "# Obs.: Como estamos treinando por-usuário, uma avaliação rigorosa exigiria validação cruzada por usuário.\n",
    "test_uid = test[test.userId == uid]\n",
    "if not test_uid.empty:\n",
    "    # previsões do classificador para itens do TESTE do uid\n",
    "    # (re-treina rapidamente o classificador para o uid; usa a mesma função interna)\n",
    "    Xc, yc, _, _ = build_user_xy(uid)\n",
    "    if Xc is not None and len(np.unique(yc)) >= 2:\n",
    "        clf = MLPClassifier(hidden_layer_sizes=(64,), max_iter=300, early_stopping=True, random_state=42)\n",
    "        clf.fit(Xc, yc)\n",
    "        Xi = X_items[test_uid.movieId.astype(int).to_numpy()]\n",
    "        p = clf.predict_proba(Xi)[:, 1]\n",
    "        y_true = (test_uid['rating'] >= 3).to_numpy()\n",
    "        y_pred = (p >= 0.5)\n",
    "        acc = (y_true == y_pred).mean()\n",
    "        from sklearn.metrics import precision_recall_fscore_support\n",
    "        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary', zero_division=0)\n",
    "        print(f\"\\nAcurácia no TESTE do usuário {uid}: {acc:.4f} | Precision: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
    "    else:\n",
    "        print(f\"\\nUsuário {uid} tem histórico de uma classe só — avaliação do classificador fica indefinida (fallback aplicado).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
